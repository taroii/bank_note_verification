<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Verifying Legitimacy of Bank Notes using Several Bank Note Factors</title>
        <link rel="icon" type="image/x-icon" href="{{url_for('static', filename='assets/favicon.ico')}}" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="{{url_for('static', filename='css/styles.css')}}" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
            <div class="container px-4">
                <a class="navbar-brand" href="#page-top">Bank Note Verification</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto">
                        <li class="nav-item"><a class="nav-link" href="#about">About</a></li>
                        <li class="nav-item"><a class="nav-link" href="#services">Exploratory Data Analysis</a></li>
                        <li class="nav-item"><a class="nav-link" href="#contact">Machine Learning</a></li>
                        <li class="nav-item"><a class = "nav-link" href="#Conclusion">Conclusion</a></li>
                        <li class="nav-item"><a class="nav-link" href="#team">Creator</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Header-->
        <header class="bg-primary bg-gradient text-white">
            <div class="container px-4 text-center">
                <h1 class="fw-bolder">Predicting Legitimacy of Bank Notes using Several Bank Note Factors</h1>
                <p class="lead">A data science project by an AI Camp Summer 2023 Data Science Intern</p>
                <a class="btn btn-lg btn-light" href="#about">Click here to learn more!</a>
            </div>
        </header>
        <!-- About section-->
        <section id="about">
            <div class="container px-4">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <h2>What does the project do?
                        </h2>
                        <p class="lead"> In this project, my goal is to predict the validity of bank notes (whether or not they're counterfeit) based on several bank note features. These features were obtained through a Wavelet Transform tool which extracted details about each bank note such as their variance, skewness, curtosis, and entropy. (<a href="https://archive.ics.uci.edu/ml/datasets/banknote+authentication">Based on UCI's Machine Learning Repository</a>) With a wide range of legitimate and counterfeit bank note data, we will create a model that can differentiate (or classify) the real from the fake! </p> 
                    </div>
                </div>
            </div>
        </section>
        <!-- Services section-->
        <section class="bg-light" id="services">
            <div class="container px-4">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                    <h1 id="top-level-heading">Exploratory Data Analysis</h1><br/>
                    </div>
                    <div class="col-lg-8">
                        <h3>Feature Distributions</h3>
                        <p class="lead">Before anything else, my first step towards understanding the data was to visualize the distributions of each variable. By looking at the side-by-side boxplots below, we can extract some valuable information, such as how the median variance (Image.Var) and median skew (Image.Skew) of bank notes differed drastically between real and fake bank notes, while median curtosis (Image.Curt) and median entropy were roughly the same between the two. On top of that, we observe more outliers in curtosis and entropy measurements, which can be potential problems when trying to us those predictors for our classification problem. </p>
                    <iframe width="900" height="900" title="Correlation Table of Features" src="{{url_for('static', filename='boxplots.html')}}"></iframe> <br>
                    </div>
                    <div class="col-lg-8">
                        <h3>Pair Plot</h3>
                        <p class="lead">Pair plots go beyond the individual distributions found in the previous figure by showcasing the relationships each feature has with the others while separating fake bank notes (class=0) and real bank notes (class=1). In general, two features that overlap a lot indicate that there can be some collinearity between the variables (ie they provide so much similar information that they don't add much to differentiating between real and fake bank notes). Here, we see some separation of class (real v. fake) between variance and skew in their density plots (diagonal plots), meaning that these predictors will be better able to differentiate between real and fake bank notes. On the otherhand, curtosis and entropy show a lot of overlap, indicating that it may be harder to use these predictors to distinguish between real and fake. </p>
                        <img width="900" height="900" title="Pairplot of Features" src="{{url_for('static', filename='images/pairplot.png')}}" /img> <br><br>
                    </div>
                    <div class="col-lg-8">
                        <h3>Correlation Heat Map</h3>
                        <p class="lead">As we stated earlier, we want to avoid colinearity between variables to ensure that our model doesn't need any unnecessary data. The benefit of reducing the number of features is to ensure that our model is running as efficiently as possible without losing too much efficacy. Another detail we can observe from this plot is how much each variable is correlated to class (whether the bank note is real or not). A higher magnitude of correlation with class implies greater predictive power of that variable. Below, we see higher correlations between variance/skew and class. This tells us that higher variance and skew of bank notes leads to a higher chance of the bank note being fake. We also see that curtosis and entropy have little to do with the class, with much lower correlations. In terms of collinearity, skew and curtosis are very related with a correlation of -0.787, and skew and entropy are also correlated with a correlation of -0.526. </p>
                        <iframe width="900" height="900" title="Correlation Heat Map" src="{{url_for('static', filename='corr_plot.html')}}"></iframe>  
                        <br>
                    </div>
                </div>
            </div>
        </section>
        <!-- Contact section-->
        <section id="contact">
            <div class="container px-4">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <h1>Machine Learning</h1></br>
                    </div>
                    <div class="col-lg-8">
                        <h2>Logistic Regression: Explanation</h2>
                        <p class="lead">First, I tried logistic regression, which is one of the most simple classification methods out there and predicts the probability that each bank note is real (i.e. closer to 1 means that there's a higher chance that that bank note is real). </p>
                        <img src="{{url_for('static', filename='images/logistic_explanation.jpg')}}" class="page-image" width = 900 height = 400/>
                    </div>

                    <div class="col-lg-8">
                        <h2>Linear Regression: Results</h2>
                        <p class="lead">Here we have a confusion matrix with the results of the logistic regression applied on our testing dataset (on a 80-20 train test split). A confusion matrix for a classification problem tells us the number of correct and incorrect predictions for every class, which in our case is real and fake bank notes. </p>
                        <img width="500" height="500" title="Logistic Regression Confusion Matrix" src="{{url_for('static', filename='images/logistic_cm.png')}}" /img>  
                        <p class="lead">Above, we see that the logistic regression model only misclassified three bank notes out of the 275 it predicted. For those three, it predicted that the bank notes were real, but they were actually fake. These are what are known as false positives, which is the type of error we want to avoid in this context because a fake bank note circulating through our economy is worse than if a real one is marked as fake. At least in the latter, we can try other verification methods to make sure that it is fake, whereas in the former, we might not get the opportunity to test it again before it gets spread around. In order to minimize this type of error, we want to maximize precision. For the logistic regression model, we obtained a precision of 97.6%.</p>
                    </div>
                    <div class="col-lg-8">
                        <h2>Extreme Gradient Boosting: Explanation</h2>
                        <p class="lead">Extreme gradient boosting (or XGBoost) is a machine learning algorithm that builds off of previously created algorithms like boosted decision trees, and makes it more efficient and accurate by controlling over-fitting and other complex techniques. </p>
                        <img src="{{url_for('static', filename='images/xgb_explanation.png')}}" class="page-image" width="800"/>
                    </div>

                    <div class="col-lg-8">
                        <h2>XGBoost: Results</h2>
                        <p class="lead">Looking at the confusion matrix below, the XGBoost model performed slightly better than the logistic regression model as it only misclassified two of the 275 bank notes.</p>
                        <img width="500" height="500" title="XGBoost Confusion Matrix" src="{{url_for('static', filename='images/xgb_cm.png')}}" /img>
                        <p class="lead">Here, we obtained a precision of 98.4%, which, again, is slightly better than the logistic regression model. </p>
                    </div>
                    <div class="col-lg-8">
                        <h2>K Nearest Neighbors: Explanation</h2>
                        <p class="lead">K Nearest Neighbors looks at data points in the testing set and infers the classification based on similarity to other points of the same class in the training set.</p>
                        <img src="{{url_for('static', filename='images/knn_explanation.jpeg')}}" class="page-image" width="500"/>
                    </div>
                    <div class="col-lg-8">
                        <h2>K Nearest Neighbors: Results</h2>
                        <p class="lead">Looking at our confusion matrix below, the KNN model performed the best out of the three models tried so far with a perfect classification rate. </p>
                        <img width="500" height="500" title="KNN Confusion Matrix" src="{{url_for('static', filename='images/knn_cm.png')}}"/img>  
                        <p class="lead">Because of this perfect classification, we have a precision of 100%. </p>
                    </div>
                  <div class="col-lg-8">
                        <h2>Random Forests: Explanation</h2>
                        <p class="lead">Random Forests, much like XGBoost, is a decision tree based algorithm that makes several, randomly selected decision trees and classifies based on the majority of those subtrees. </p>
                        <img src="{{url_for('static', filename='images/rf_explanation.png')}}" class="page-image" width="800"/>
                    </div>
                    <div class="col-lg-8">
                        <h2>Random Forests: Results</h2>
                        <p class="lead">Looking at our confusion matrix below, the random forests model performed the same as the XGBoost model with only two misclassifications. </p>
                        <iframe width="500" height="500" title="Random Forests Confusion Matrix" src="{{url_for('static', filename='images/rf_cm.png')}}"></iframe>  
                        <p class="lead">Similarly to the XGBoost model, we have a precision of 98.4%. </p>
                    </div>
                </div>
            </div>
        </section>
        <section class="bg-light" id="Conclusion">
            <div class="container px-4">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                    <h1 id="top-level-heading">Conclusion</h1><br/>
                    </div>
                    <div class="col-lg-8">
                        <iframe width="900" height="500" title="Model Evaluations" src="{{url_for('static', filename='model_specs.html')}}"></iframe>
                        <p class="lead">As we can see, kNN outperformed the three other models with a perfect accuracy and precision. While this difference may be insignificant (due to random chance), all four of the models have a very high accuracy and precision, meaning that they are all pretty good options. One benefit of using KNN for this purpose is its ease of explainability, and since we have a relatively small dataset to work on, we don't have to worry about efficiency issues that KNN models tend to suffer from due to large data sets with high dimensionality. One thing we can take advantage of from the random forest model, however, is that it measures the importance of each feature in the dataset. </p>

                      <iframe width="900" height="500" title="Variable Importances" src="{{url_for('static', filename='importances.html')}}"></iframe>
                      
                      <p class="lead">Just like we implied in the EDA, we can see above that the importance of variance and skew are higher than curtosis and entropy, meaning that the variance and skew of bank note images play a bigger role in determining the validity of the bank note.
                      </p>

                      <p class="lead">One way we can improve this model is by removing the limitation of wavelet transformed images. By training the model on regular, unprocessed images, we can have a more functional and useful product that anyone can use. Another way we can improve this model is by training it on a larger dataset. 
                      </p>

                        <h3>So what?</h3>
                            
                        <p class="lead">So what is the significance of our project? By creating several, high-performing models that can differentiate between real and fake bank notes based on wavelet transformed images, we can easily scan hundreds of bank notes and classify them in seconds. Since it's based on images, these models can be implemented in computer vision applications, where we can actively scan and identify real and fake bank notes in real time. </p>
                    </div>
                </div>
            </div>
        </section>

        <section style="margin-left: 80px; margin-right: 80px;" id="team">
        <header class="bg-primary text-center py-5 mb-4">
            <div class="container px-4">
              <h1 class="fw-light text-white">Meet the Creator :)</h1>
            </div>
          </header>
          
          <!-- Page Content -->
          <div class="container px-4">
            <div class="row">
              <!-- Team Member 1 -->
              <div class="d-flex justify-content-center">
                <div class="col-xl-3 col-md-6 mb-4">
                  <div class="card border-0 shadow">
                    <img src="{{url_for('static', filename='images/taro.jpg')}}" class="card-img-top" alt="...">
                    <div class="card-body text-center">
                      <h5 class="card-title mb-0">Taro Iyadomi</h5>
                      <div class="card-text text-black-50">Data Scientist @ UCLA</div>
                      <div class="card-text text-black-50">AI Camp Summer 23 Intern</div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            <!-- /.row -->
          
          </div>
        </section>
        <!-- Footer-->
        <footer class="py-5 bg-dark">
            <div class="container px-4"><p class="m-0 text-center text-white">Copyright &copy; AI Camp 2022</p></div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="/static/js/scripts.js"></script>
    </body>
</html>
2>